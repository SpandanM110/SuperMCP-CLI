# {{projectName}}

MCP server for {{docsName}} documentation. Generated by [Super MCP](https://www.npmjs.com/package/super-mcp).

## Quick Start

```bash
npm install
npm run build
npm start
```

## LLM Configuration (BYOK)

Supports **local** (Ollama) and **cloud** providers. Set `LLM_PROVIDER` and the appropriate API key:

| Provider | Env Var | Models |
|----------|---------|--------|
| ollama | (none) | llama3.2, mistral, etc. |
| openai | OPENAI_API_KEY | gpt-4, gpt-3.5-turbo |
| anthropic | ANTHROPIC_API_KEY | claude-3-5-sonnet, claude-3-opus |
| groq | GROQ_API_KEY | llama-3.1-70b, mixtral-8x7b |
| together | TOGETHER_API_KEY | meta-llama/Llama-3-70b |
| mistral | MISTRAL_API_KEY | mistral-large, codestral |

```bash
# Local (default)
LLM_PROVIDER=ollama
LLM_MODEL=llama3.2

# Cloud (BYOK)
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-...
LLM_MODEL=claude-3-5-sonnet-20241022
```

## Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| LLM_PROVIDER | ollama, openai, anthropic, groq, together, mistral | ollama |
| LLM_ENDPOINT | Override endpoint (local) | http://localhost:11434/api/generate |
| LLM_MODEL | Model name | {{llmConfig.model}} |
| CONTEXT_PATH | Path to docs.json | ./context/docs.json |
| LOG_LEVEL | Log level | info |

## Tools

- **ask_docs** - Ask questions about {{docsName}} documentation
- **search_docs** - Search through documentation for topics/keywords
- **generate_code** - Generate code from docs (guides LLM in right direction)

## Archestra Integration

```yaml
Name: {{serverName}}
Transport: stdio
Command: docker
Args:
  - run
  - --rm
  - -i
  - --network=host
  - -e
  - LLM_PROVIDER=${LLM_PROVIDER}
  - -e
  - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
  - {{projectName}}:latest
```

Or run: `super-mcp export-archestra` from this directory.

## Claude Desktop

Add to `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "{{serverName}}": {
      "command": "node",
      "args": ["/ABSOLUTE/PATH/TO/{{projectName}}/dist/index.js"],
      "env": {
        "LLM_PROVIDER": "ollama",
        "LLM_MODEL": "{{llmConfig.model}}"
      }
    }
  }
}
```

Or run: `super-mcp add-to-claude` from this directory.

---
Generated at {{generatedAt}}
