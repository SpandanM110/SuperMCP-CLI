#!/usr/bin/env python3

import asyncio
import json
import logging
import os
import sys
from pathlib import Path
from typing import Any, Dict, List

import httpx
from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import Tool, TextContent

logging.basicConfig(
    level=os.getenv("LOG_LEVEL", "INFO").upper(),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stderr)],
)
logger = logging.getLogger(__name__)

SERVER_NAME = "{{serverName}}"
VERSION = "1.0.0"
LLM_ENDPOINT = os.getenv("LLM_ENDPOINT", "http://localhost:11434/api/generate")
LLM_MODEL = os.getenv("LLM_MODEL", "{{llmConfig.model}}")
CONTEXT_PATH = os.getenv("CONTEXT_PATH", "context/docs.json")


class ContextManager:
    def __init__(self, context_path: str):
        self.context_path = Path(context_path)
        self.docs: Dict[str, Any] = {}

    async def initialize(self):
        try:
            with open(self.context_path, "r") as f:
                self.docs = json.load(f)
            logger.info(f"Loaded {self.docs.get('pageCount', 0)} documentation pages")
        except Exception as e:
            logger.error(f"Failed to load documentation: {e}")
            raise

    def search(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        query_lower = query.lower()
        keywords = [k for k in query_lower.split() if len(k) > 2]

        scored = []
        for page in self.docs.get("pages", []):
            content_lower = (page.get("title", "") + " " + page.get("content", "")).lower()
            score = 0

            if query_lower in content_lower:
                score += 100

            for keyword in keywords:
                matches = content_lower.count(keyword)
                score += matches * 10
                if keyword in page.get("title", "").lower():
                    score += 50

            if score > 0:
                scored.append({"page": page, "score": score})

        scored.sort(key=lambda x: x["score"], reverse=True)
        return [s["page"] for s in scored[:max_results]]


class LLMClient:
    def __init__(self, endpoint: str, model: str):
        self.endpoint = endpoint
        self.model = model
        self.client = httpx.AsyncClient(timeout=30.0)
        self.is_ollama = "ollama" in endpoint

    async def query(self, prompt: str) -> str:
        try:
            if self.is_ollama:
                response = await self.client.post(
                    self.endpoint,
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {"temperature": 0.7, "top_p": 0.9},
                    },
                )
                response.raise_for_status()
                return response.json()["response"]
            else:
                response = await self.client.post(
                    self.endpoint,
                    json={
                        "model": self.model,
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.7,
                        "max_tokens": 2000,
                    },
                )
                response.raise_for_status()
                return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"LLM query failed: {e}")
            raise

    async def test_connection(self) -> bool:
        try:
            result = await self.query("Say 'OK' and nothing else.")
            return "ok" in result.lower()
        except Exception:
            return False


def build_prompt(
    question: str, docs: List[Dict[str, Any]], include_examples: bool
) -> str:
    docs_context = "\n\n---\n\n".join(
        [f"# {doc.get('title', '')}\n\n{doc.get('content', '')}" for doc in docs]
    )
    examples_instruction = (
        "Include code examples where applicable"
        if include_examples
        else "Focus on explanations without code examples"
    )
    return f"""You are an expert on {{docsName}}. Use the following official documentation to provide accurate, helpful answers.

DOCUMENTATION:
{docs_context}

QUESTION: {question}

INSTRUCTIONS:
- Provide accurate information based solely on the documentation above
- {examples_instruction}
- If the documentation doesn't contain relevant information, say so
- Be concise but thorough
- Format code with markdown code blocks

ANSWER:"""


async def main():
    context_manager = ContextManager(CONTEXT_PATH)
    await context_manager.initialize()

    llm_client = LLMClient(LLM_ENDPOINT, LLM_MODEL)
    if not await llm_client.test_connection():
        logger.error("LLM connection test failed")
        sys.exit(1)
    logger.info("LLM connection verified")

    server = Server(SERVER_NAME)

    @server.list_tools()
    async def list_tools() -> List[Tool]:
        return [
            Tool(
                name="ask_docs",
                description="Ask questions about {{docsName}} documentation",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "question": {"type": "string"},
                        "include_examples": {"type": "boolean", "default": True},
                    },
                    "required": ["question"],
                },
            ),
            Tool(
                name="search_docs",
                description="Search {{docsName}} documentation",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "query": {"type": "string"},
                        "max_results": {"type": "number", "default": 5},
                    },
                    "required": ["query"],
                },
            ),
        ]

    @server.call_tool()
    async def call_tool(name: str, arguments: Dict[str, Any]) -> List[TextContent]:
        logger.info(f"Tool called: {name}")

        if name == "ask_docs":
            question = arguments["question"]
            include_examples = arguments.get("include_examples", True)
            relevant_docs = context_manager.search(question, 5)
            prompt = build_prompt(question, relevant_docs, include_examples)
            answer = await llm_client.query(prompt)
            return [TextContent(type="text", text=answer)]

        elif name == "search_docs":
            query = arguments["query"]
            max_results = arguments.get("max_results", 5)
            results = context_manager.search(query, max_results)
            formatted = "\n\n---\n\n".join(
                [
                    f"## Result {i+1}: {doc.get('title', '')}\n\nURL: {doc.get('url', '')}\n\n{doc.get('content', '')[:500]}..."
                    for i, doc in enumerate(results)
                ]
            )
            return [TextContent(type="text", text=formatted)]

        else:
            raise ValueError(f"Unknown tool: {name}")

    async with stdio_server() as (read_stream, write_stream):
        logger.info(f"{SERVER_NAME} MCP server running")
        await server.run(read_stream, write_stream, server.create_initialization_options())


if __name__ == "__main__":
    asyncio.run(main())
